{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNING PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_300 = r\"C:\\Users\\Owner\\Desktop\\Hyperoparameter CSV\\Final Json\\hyperparameter-db-project-ds03-master\\nRidYg95h 300\\*.json\"\n",
    "path_500 = r\"C:\\Users\\Owner\\Desktop\\Hyperoparameter CSV\\Final Json\\hyperparameter-db-project-ds03-master\\XBB2YBVDt 500\\*.json\"\n",
    "path_800 = r\"C:\\Users\\Owner\\Desktop\\Hyperoparameter CSV\\Final Json\\hyperparameter-db-project-ds03-master\\douhJzEiy 800\\*.json\"\n",
    "path_1000 = r\"C:\\Users\\Owner\\Desktop\\Hyperoparameter CSV\\Final Json\\hyperparameter-db-project-ds03-master\\uh5qmPqcn 1000\\*.json\"\n",
    "path_1200 = r\"C:\\Users\\Owner\\Desktop\\Hyperoparameter CSV\\Final Json\\hyperparameter-db-project-ds03-master\\9Zo6W4yCU 1200\\*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_300 = glob.glob(path_300)\n",
    "files_500 = glob.glob(path_500)\n",
    "files_800 = glob.glob(path_800)\n",
    "files_1000 = glob.glob(path_1000)\n",
    "files_1200 = glob.glob(path_1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={\"DRF\":[ \"fold_assignment\",\"fold_column\" ,\"offset_column\" ,\"weights_column\" ,\n",
    "             \"balance_classes\" ,\"class_sampling_factors\" ,\"max_after_balance_size\" ,\"ntrees\",\n",
    "             \"max_depth\" ,\"min_rows\" ,\"nbins\",\"nbins_top_level\" ,\"nbins_cats\" ,\"stopping_rounds\" ,\n",
    "             \"stopping_metric\" ,\"stopping_tolerance\" ,\"max_runtime_secs\" ,\"seed\" ,\"mtries\" ,\n",
    "             \"sample_rate\" ,\"sample_rate_per_class\" ,\"col_sample_rate_change_per_level\" ,\"col_sample_rate_per_tree\" ,\n",
    "             \"min_split_improvement\", \"histogram_type\" ,\"categorical_encoding\",\"model_id\"],\n",
    "      \"GBM\":[\"fold_assignment\" ,\"fold_column\" , \"offset_column\" ,\"weights_column\" ,\"class_sampling_factors\" , \n",
    "             \"max_after_balance_size\" ,\"ntrees\" ,\"min_rows\" ,\"nbins\" , 'nbins_top_level' ,\"nbins_cats\" ,\"stopping_rounds\" ,\n",
    "             \"stopping_metric\" ,\"stopping_tolerance\" ,\"max_runtime_secs\" ,\"seed\" ,\"learn_rate\" ,\"learn_rate_annealing\" ,  \n",
    "             \"distribution\" ,\"sample_rate\" ,\"sample_rate_per_class\" ,\"col_sample_rate\" ,\"col_sample_rate_change_per_level\" ,    \n",
    "             \"col_sample_rate_per_tree\" ,\"min_split_improvement\" ,\"histogram_type\" ,\"max_abs_leafnode_pred\" ,\"pred_noise_bandwidth\"],\n",
    "     \"GLM\": [\"seed\", \"fold_assignment\", \"fold_column\", \"offset_column\", \"weights_column\", \"tweedie_variance_power\",\n",
    "             \"theta\" , \"alpha\", \"lambda\", \"standardize\", \"missing_values_handling\", \"max_runtime_secs\"],\n",
    "     \"XGBoost\":['weights_column' ,'offset_column' ,'fold_column' ,'fold_assignment' ,\n",
    "                'stopping_rounds', 'max_runtime_secs' ,'stopping_metric' ,'stopping_tolerance' ,\n",
    "                \"ntrees\" ,\"max_depth\" ,\"min_row\" , \"seed\" ,\"sample_rate\" ,'sub_sample' ,\n",
    "                \"col_sample_rate\" ,\"col_sample_by_level\" ,\"col_sample_rate_per_tree\" ,\n",
    "                \"colsample_bytree\" ,\"min_split_improvement\" ,\"gamma\" ,\"learn_rate\" , \"eta\" ,\\\n",
    "                \"max_abs_leafnode_pred\" ,\"max_delta_step\" , \"distribution\" , \"tweedie_power\" , \"categorical_encoding\" ,\n",
    "                \"tree_method\" , \"num_leaves\" , \"min_sum_hessian_in_leaf\" ,\"min_data_in_leaf\" ,\"grow_policy\" ,\"booster\" ,\n",
    "                \"reg_lambda\" , \"sample_type\" , \"normalize_type\" , \"rate_drop\" , \"one_drop \",\"skip_drop\" ]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_XGBoost5=pd.DataFrame(columns=dict[\"XGBoost\"])\n",
    "hp_GLM5=pd.DataFrame(columns=dict[\"GLM\"])\n",
    "hp_GBM5=pd.DataFrame(columns=dict[\"GBM\"])\n",
    "hp_DRF5=pd.DataFrame(columns=dict[\"DRF\"])\n",
    "temp_name_xgb=[]\n",
    "temp_name_gbm=[]\n",
    "temp_name_glm=[]\n",
    "temp_name_drf=[]\n",
    "\n",
    "for f in files_500:\n",
    "    if \"XGBoost\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_xgb.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"XGBoost\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_XGBoost5=pd.concat([hp_XGBoost5,data3],join=\"inner\")\n",
    "        hp_XGBoost5[\"runtime\"]=500\n",
    "        hp_XGBoost5[\"name\"]=temp_name_xgb\n",
    "    elif \"GLM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_glm.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GLM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GLM5=pd.concat([hp_GLM5,data3],join=\"inner\")\n",
    "        hp_GLM5[\"runtime\"]=500\n",
    "        hp_GLM5[\"name\"]=temp_name_glm\n",
    "    elif \"GBM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_gbm.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GBM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GBM5=pd.concat([hp_GBM5,data3],join=\"inner\")\n",
    "        hp_GBM5[\"runtime\"]=500\n",
    "        hp_GBM5[\"name\"]=temp_name_gbm\n",
    "    \n",
    "    elif \"DRF\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_drf.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"DRF\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_DRF5=pd.concat([hp_DRF5,data3],join=\"inner\")\n",
    "        hp_DRF5[\"runtime\"]=500\n",
    "        hp_DRF5[\"name\"]=temp_name_drf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_XGBoost3=pd.DataFrame(columns=dict[\"XGBoost\"])\n",
    "hp_GLM3=pd.DataFrame(columns=dict[\"GLM\"])\n",
    "hp_GBM3=pd.DataFrame(columns=dict[\"GBM\"])\n",
    "hp_DRF3=pd.DataFrame(columns=dict[\"DRF\"])\n",
    "temp_name_xgb3=[]\n",
    "temp_name_gbm3=[]\n",
    "temp_name_glm3=[]\n",
    "temp_name_drf3=[]\n",
    "\n",
    "for f in files_300:\n",
    "    if \"XGBoost\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_xgb3.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"XGBoost\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_XGBoost3=pd.concat([hp_XGBoost3,data3],join=\"inner\")\n",
    "        hp_XGBoost3[\"runtime\"]=300\n",
    "        hp_XGBoost3[\"name\"]=temp_name_xgb3\n",
    "    elif \"GLM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_glm3.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GLM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GLM3=pd.concat([hp_GLM3,data3],join=\"inner\")\n",
    "        hp_GLM3[\"runtime\"]=300\n",
    "        hp_GLM3[\"name\"]=temp_name_glm3\n",
    "    elif \"GBM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_gbm3.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GBM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GBM3=pd.concat([hp_GBM3,data3],join=\"inner\")\n",
    "        hp_GBM3[\"runtime\"]=300\n",
    "        hp_GBM3[\"name\"]=temp_name_gbm3\n",
    "    \n",
    "    elif \"DRF\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_drf3.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"DRF\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_DRF3=pd.concat([hp_DRF3,data3],join=\"inner\")\n",
    "        hp_DRF3[\"runtime\"]=300\n",
    "        hp_DRF3[\"name\"]=temp_name_drf3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_XGBoost8=pd.DataFrame(columns=dict[\"XGBoost\"])\n",
    "hp_GLM8=pd.DataFrame(columns=dict[\"GLM\"])\n",
    "hp_GBM8=pd.DataFrame(columns=dict[\"GBM\"])\n",
    "hp_DRF8=pd.DataFrame(columns=dict[\"DRF\"])\n",
    "temp_name_xgb8=[]\n",
    "temp_name_gbm8=[]\n",
    "temp_name_glm8=[]\n",
    "temp_name_drf8=[]\n",
    "\n",
    "for f in files_800:\n",
    "    if \"XGBoost\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_xgb8.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"XGBoost\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_XGBoost8=pd.concat([hp_XGBoost8,data3],join=\"inner\")\n",
    "        hp_XGBoost8[\"runtime\"]=800\n",
    "        hp_XGBoost8[\"name\"]=temp_name_xgb8\n",
    "    elif \"GLM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_glm8.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GLM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GLM8=pd.concat([hp_GLM8,data3],join=\"inner\")\n",
    "        hp_GLM8[\"runtime\"]=800\n",
    "        hp_GLM8[\"name\"]=temp_name_glm8\n",
    "    elif \"GBM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_gbm8.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GBM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GBM8=pd.concat([hp_GBM8,data3],join=\"inner\")\n",
    "        hp_GBM8[\"runtime\"]=800\n",
    "        hp_GBM8[\"name\"]=temp_name_gbm8\n",
    "    \n",
    "    elif \"DRF\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_drf8.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"DRF\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_DRF8=pd.concat([hp_DRF8,data3],join=\"inner\")\n",
    "        hp_DRF8[\"runtime\"]=800\n",
    "        hp_DRF8[\"name\"]=temp_name_drf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_XGBoost10=pd.DataFrame(columns=dict[\"XGBoost\"])\n",
    "hp_GLM10=pd.DataFrame(columns=dict[\"GLM\"])\n",
    "hp_GBM10=pd.DataFrame(columns=dict[\"GBM\"])\n",
    "hp_DRF10=pd.DataFrame(columns=dict[\"DRF\"])\n",
    "temp_name_xgb10=[]\n",
    "temp_name_gbm10=[]\n",
    "temp_name_glm10=[]\n",
    "temp_name_drf10=[]\n",
    "\n",
    "for f in files_1000:\n",
    "    if \"XGBoost\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_xgb10.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"XGBoost\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_XGBoost10=pd.concat([hp_XGBoost10,data3],join=\"inner\")\n",
    "        hp_XGBoost10[\"runtime\"]=1000\n",
    "        hp_XGBoost10[\"name\"]=temp_name_xgb10\n",
    "    elif \"GLM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_glm10.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GLM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GLM10=pd.concat([hp_GLM10,data3],join=\"inner\")\n",
    "        hp_GLM10[\"runtime\"]=1000\n",
    "        hp_GLM10[\"name\"]=temp_name_glm10\n",
    "    elif \"GBM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_gbm10.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GBM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GBM10=pd.concat([hp_GBM10,data3],join=\"inner\")\n",
    "        hp_GBM10[\"runtime\"]=1000\n",
    "        hp_GBM10[\"name\"]=temp_name_gbm10\n",
    "    \n",
    "    elif \"DRF\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_drf10.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"DRF\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_DRF10=pd.concat([hp_DRF10,data3],join=\"inner\")\n",
    "        hp_DRF10[\"runtime\"]=1000\n",
    "        hp_DRF10[\"name\"]=temp_name_drf10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_XGBoost12=pd.DataFrame(columns=dict[\"XGBoost\"])\n",
    "hp_GLM12=pd.DataFrame(columns=dict[\"GLM\"])\n",
    "hp_GBM12=pd.DataFrame(columns=dict[\"GBM\"])\n",
    "hp_DRF12=pd.DataFrame(columns=dict[\"DRF\"])\n",
    "temp_name_xgb12=[]\n",
    "temp_name_gbm12=[]\n",
    "temp_name_glm12=[]\n",
    "temp_name_drf12=[]\n",
    "\n",
    "for f in files_1200:\n",
    "    if \"XGBoost\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_xgb12.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"XGBoost\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_XGBoost12=pd.concat([hp_XGBoost12,data3],join=\"inner\")\n",
    "        hp_XGBoost12[\"runtime\"]=1200\n",
    "        hp_XGBoost12[\"name\"]=temp_name_xgb12\n",
    "    elif \"GLM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_glm12.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GLM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GLM12=pd.concat([hp_GLM12,data3],join=\"inner\")\n",
    "        hp_GLM12[\"runtime\"]=1200\n",
    "        hp_GLM12[\"name\"]=temp_name_glm12\n",
    "    elif \"GBM\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_gbm12.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"GBM\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_GBM12=pd.concat([hp_GBM12,data3],join=\"inner\")\n",
    "        hp_GBM12[\"runtime\"]=1200\n",
    "        hp_GBM12[\"name\"]=temp_name_gbm12\n",
    "    \n",
    "    elif \"DRF\" in f:\n",
    "        data1=pd.read_json(f)\n",
    "        aa=data1['model_id']['actual']['name']\n",
    "        temp_name_drf12.append(aa)\n",
    "        datat=data1.transpose()\n",
    "        data1[\"model_id\"]=datat.actual[\"model_id\"][\"name\"]\n",
    "        data1[\"training_frame\"]=datat.actual[\"training_frame\"][\"name\"]\n",
    "        data2=data1[dict[\"DRF\"]].copy()\n",
    "        data3=data2.drop(\"default\")\n",
    "        hp_DRF12=pd.concat([hp_DRF12,data3],join=\"inner\")\n",
    "        hp_DRF12[\"runtime\"]=1200\n",
    "        hp_DRF12[\"name\"]=temp_name_drf12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRF_FINAL_TABLE  = hp_DRF3.append(hp_DRF5.append(hp_DRF8.append(hp_DRF10.append(hp_DRF12))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBM_FINAL_TABLE  = hp_GBM3.append(hp_GBM5.append(hp_GBM8.append(hp_GBM10.append(hp_GBM12))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_FINAL_TABLE  = hp_GLM3.append(hp_GLM5.append(hp_GLM8.append(hp_GLM10.append(hp_GLM12))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_FINAL_TABLE  = hp_XGBoost3.append(hp_XGBoost5.append(hp_XGBoost8.append(hp_XGBoost10.append(hp_XGBoost12))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRF_FINAL_TABLE.to_csv(\"DRF_FINAL_TABLE.csv\")\n",
    "GBM_FINAL_TABLE.to_csv(\"GBM_FINAL_TABLE.csv\")\n",
    "GLM_FINAL_TABLE.to_csv(\"GLM_FINAL_TABLE.csv\")\n",
    "XGB_FINAL_TABLE.to_csv(\"XGB_FINAL_TABLE.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
